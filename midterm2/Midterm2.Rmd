---
title: "Midterm2 Stat 151A, Spring 2020"
author: "Kanak Garg"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup,include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(SignifReg)
library(leaps)
library(gpairs)
require(lars)
library(parcor)
```

# Introduction

Capital Bikeshare System is a bike rental service in our countries capital, at Washington D.C. Luckily, Capital Bikeshare System has been collecting information over the course of the last two years regarding their rental service; gathering information such as the date, the season, the weather conditions and more. In this report, we will touch on using this information to see if we can accurately predict bike rentals in a given hour using this information.

#Data Description

Each row of the data structure correlates to the amount of bike rentals for a specific hour over the course of two years. The row comprises of different variables corresponding to time and weather. Time is divided up by the different specifications for the time of the purchase -- for example the month, year, season, year. The data also tracks the weather, as different weather conditions that may affect bike rental. Many of the columns in the Bike Rental Data are markers, such as season or year, that are incorrectly configured as integers. In order for the data to be usable, these columns must be converted from integer values to the type factor. The DTEDAY column must be converted to a Date format, as it is a Date. 


```{r clean-data, echo = FALSE}
bike <- read.csv("BikeSharingDataset.csv")

#must convert columns to the correct data type in order to use it properly in linear modeling.  
bike[,'dteday']<-as.Date(bike[,'dteday'])
#Factor > Date



bike[,'mnth']<-factor(bike[,'mnth'])
bike[,'hr']<-factor(bike[,'hr'])
bike[,'holiday']<-factor(bike[,'holiday'])
bike[,'weekday']<-factor(bike[,'weekday'])
#bike[,'season']<-factor(bike[,'season'])
bike[,'workingday']<-factor(bike[,'workingday'])
bike[,'weathersit']<-factor(bike[,'weathersit'])
#Ints > Factors

head(bike)

```


#Exploratory Data Analysis

Of the 17 data variables that are given to us in the Bike Rental Data, we can logically eliminate four of the columns in order to end up with a remaining working data set. The columns INSTANT is a unique number that corresponds to a row of the dataset, which is useful in data management, however unneccessary for linear modeling. The column dteday is very useful as well as it tells us the date for every purchase, however the rest of our time indicator variables are sufficient and makes this column unneccessary. The last two columns requiring analysis are columns registered and casual, which correspond to the type of customer making a rental. Since CNT = Casual + Registered, a linear model of CNT would be equivalent to making a linear model of Casual and linear model of Registered and adding it together. Thus we can further clean data by removing these two columns.

Further inspecting the dataset, we must divide the variables into categorical and continous variables. We must inspect both to determine the validity of these input variables through univariate and bivariate variable analysis. 

###Univariate Variable Analysis
There were some important charcteristics about the data that stood out during univariate data analysis. The rest of the continuous data types seemed to have no skew, thus are fairly normal. Another interesting characteristic of the data is the variable HOLIDAY, which has over 16000 datapoints for non holidayss, and 500 datapoints for holidays. This is not ideal as we would like to have more HOLIDAY data values to make a more sound determination. The rest of the categorical variables have an even distribution.

```{r,  echo = FALSE}
summary(bike)

```

###Bivariate Variable Analysis
Onto a bivariate analysis of the variables. We see a paired similarity between the variables SEASON and MONTH. Graphing the boxplot of both, shows that month may be a better categorical variable to use as there seems to be differences between months in a season, however not in seasons in months (only a few months have multiple seasons per month, most only have one). Using the pairs graph we analysze all the continuous variables below. As in Figure 1, there seems to be correlation between TEMP and ATEMP. Since ATEMP has a smaller z-statistic, we decided to use ATEMP. 

Analyzing the paired relationship between WEEKDAY,WORKINGDAY and HOLIDAY, I believe we condense the variables to only one or two. WEEKDAYs is the broadest variable, while WORKINGDAY and HOLIDAY gives us more information. The difference in average rentals for WORKINGDAY and HOLIDAY seem to be significant enough to keep in the data. Furthermore from the pair analysis, we should remove WEEKDAYS, however we dont have enough information from the data and should wait till model selection to decide.

```{r,  echo = FALSE, fig.cap="\\label{fig:vars} Figure 1: Paired plot of all the the weather data, which helps determine which of the weather data can be used to interpret CNT."}
#mnth vs season
#boxplot(cnt ~ mnth + season , data = bike)
#temp vs atemp
 
suppressMessages(gpairs(bike[,c("atemp", "hum", "windspeed","temp","weathersit")]))
```


```{r,  echo = FALSE}
#primarily need to extract data columns that we can work with
#columns -> (instant, dteday) cannot be used in linear model calculation
#column -> causal + registered = cnt
# we could potentially do two linear models, one for causal one for registered, howevever the addition of those models is the same as writing one linear model for cnt
newCol = names(bike) %in% c("instant", "dteday", "casual", "registered","cnt", "season", "temp")
rntDAT = bike[!newCol]
rntDAT$cnt = bike$cnt
#rntDAT$cnt = bike$cnt^(0.5)
rntDAT$cnt = bike$cnt
#par(mfrow = c(3, 3))
#for(i in names(rntDAT)){
#    if (i == "cnt"){
#      break;
#    }
#    plot(rntDAT[,i],rntDAT$cnt, xlab = i)
#}
```


#Model Selection
Now that we have narrowed down the parameters to a usable set of variables, we want to tranform our data and determine which of these variables are necessary in order to determine an accurate model.


###Transforming CNT

After removing the variables we talked about above, we must look at CNT relationship with the input variables. A plot of the fully fitted linear model is below in Figure 2. Looking at the outputted graphs, we see a slight curve in the residuals as well as a a graph that lacks fit in the initial and end of the q-q plot. The scale location model seems to increase toward the end of the graph, hinting at a lack of homoscedascity. The last residuals vs leverage graph shows us 3 data points that are high leverage and could thus be influential points.


```{r,  echo = FALSE, fig.cap="\\label{fig:lm} Figure 2: Fully fitted data model of Count data using all explanatory variables provided in the dataset."}
full_fit <- lm(cnt ~ . , data = rntDAT)
par(mfrow = c(2, 2))
plot(full_fit)
rntDAT$cnt = log(rntDAT$cnt)
rntDAT = rntDAT[-c(586,9124,8855),]
```

The best way to deal with this is by taking a log of the CNT data.


###Influential Points
In the linear model of Figure 2, we also see three important high leverage points at (586,9124,8855). These points are not necessarily a bad sign - however in order to determine a correct model, we remove these points in order to prevent any change they may have on the data. However, as they properly align with the rest of the data, as seen in the Residual vs Leverage graph, there is a low chance that they will severly impact our linear regression.

```{r,  echo = FALSE, fig.cap="\\label{fig:lmtrans} Figure 3: Graphing the linear model after making transformations to output variables and removing leverage points."}
full_fit <- lm(cnt ~ . , data = rntDAT)
par(mfrow = c(2, 2))
plot(full_fit)

```

The resulting linear model, Figure 3, is a lot better as the Residual vs Fitted graph are more centered around the mean and the Residuals vs Leverage is more linear. Some points of concern include a dip in Scale-Location graph toward later values, and a lag in initial values in the Q-Q Plot. This may indicate slight heteroscedascity, where values have less variance almost halfway through the dataset. Also our data seems to be less normal in the initial half of the dataset. Since the data is divided into two, as seen in the plot of CNT, as well as the Q-Q plot and Scale-Location, there may be some interaction present with the variable YR as it is the only variable that divides the dataset in half.


###Forward Selection

We have many options for determining such model -- including forward selection, backward selection, stepwise selection or even Lasso and Ridge regression. In this model we determine to use a forward selection model as it seems the most appropiate and aggregate tests over many different factors. Though backward selection would also be appropiate, attempting to run Lasso and Ridge did not seem to be successful due to the presence of many categorical variables in the data. 
 
In forward selection, the best way to determine if a model is appropiate is to start with an empty model and filter in variables based on which seems to be the most appropiate to the model. We use many different criteria -- p-values, AIC, BIC, Mallows C'p and Adjusted R^2. The way we perform forward selection is by creating a model and adding the best variable one at a time as long as the criteria is met. The set of variables we decided to use are the basic variables in the data as well as year's interaction with the other time categorical variables.

Based on every criteria that we perform forward selection, we end up with the same model. The best model for predicting the data seems to contain all input variables as well as time variables and their interaction with YEAR. We end up with 3 main models --

```{r warning=FALSE,  echo = FALSE}
vars = 1:14 


#basic format for these is to start w a blank model and incrmementally add values to it until the threshold doesnt hold for data
vecName = c(names(rntDAT), "yr*mnth", "yr*hr", "yr*weekday","yr*holiday", "yr*workingday")
m2 = NULL 
start2 = 'cnt ~ 1' 
while(TRUE){
  small = lm(start2, data = rntDAT)
  pvals = array(0, dim = length(vars)) 
  for (i in 1:length(vars)){
    big = lm(paste(start2, ' + ', vecName[vars[i]]), data = rntDAT)
    pvals[i] = anova(small,big)[2,6] 
    if(is.na(pvals)){
      break
    }
  }
  if(is.na(pvals)){
      break
  }
  
  best = which.min(pvals)
  
  if (pvals[best] < 0.05){
    start2 = paste(start2, ' + ', vecName[vars[best]]) 
    m2 = append(m2, vecName[vars[best]]) 
    vars = vars[-best]
  } else { 
    break
  }
}

```

```{r warning=FALSE,  echo = FALSE}
m3 = NULL 
vars = 1:14
start3 = 'cnt ~ 1' 

while(TRUE){
  smallr2 = summary(lm(start3, data = rntDAT))$adj.r.squared 
  bigr2 = array(0, dim = length(vars))
  for (i in 1:length(vars)){
    bigr2[i] = summary(lm(paste(start3, ' + ', vecName[vars[i]]), data  = rntDAT))$adj.r.squared 
  }
  best = which.max(bigr2)
  if (bigr2[best] > smallr2){
    start3 = paste(start3, ' + ', vecName[vars[best]]) 
    m3 = append(m3, x = vecName[vars[best]]) 
    vars = vars[-best]
  } 
  else { 
    break
  } 
}

```

```{r warning=FALSE,  echo = FALSE}
m4 = NULL 
vars = 1:14
start4 = 'cnt ~ 1' 
while(TRUE){
  saic = extractAIC(lm(start4, data = rntDAT))[2]
  baic = array(0, dim = length(vars))
  for (i in 1:length(vars)){
    baic[i] = extractAIC(lm(paste(start4, ' + ', vecName[vars[i]]), data = rntDAT))[2] 
  }
  best = which.min(baic)
  if (baic[best] < saic){
    start4= paste(start4, ' + ', vecName[vars[best]]) 
    m4 = append(m4, vecName[vars[best]]) 
    vars = vars[-best]
  } else { 
    break
  } 
}

```

```{r warning=FALSE,  echo = FALSE}
m5 = NULL 
vars = 1:14
start = 'cnt ~ 1' 
while(TRUE){
  sbic = extractAIC(lm(start, data = rntDAT), k = log(nrow(rntDAT)))[2] 
  bbic = array(0, dim = length(vars))
  for (i in 1:length(vars)){
    bbic[i] = extractAIC(lm(paste(start, ' + ', vecName[vars[i]]), data = rntDAT), k = log(nrow(rntDAT)))[2] 
  }
  best = which.min(bbic)
  if (bbic[best] < sbic){
    start = paste(start, ' + ', vecName[vars[best]]) 
    m5 = append(m5, vecName[vars[best]]) 
    vars = vars[-best]
  } else { 
    break
  } 
}
```

```{r warning=FALSE,  echo = FALSE}
m6 = NULL 
vars = 1:14
start = 'cnt ~ 1'
sig2hatFull =summary(lm(cnt ~ ., data = rntDAT))$sigma^2
n = nrow(rntDAT)

while(TRUE){
  sumObj =summary(lm(start, data = rntDAT))
  modDF = sumObj$df[1]
  residDF = sumObj$df[2]
  smallMallow = 2*modDF - n + sumObj$sigma^2 * residDF /sig2hatFull 
  bigM = array(0, dim = length(vars))
  for (i in 1:length(vars)){
    sumObj =summary(lm(paste(start, ' + ', vecName[vars[i]]), data = rntDAT))
    modDF = sumObj$df[1]
    residDF = sumObj$df[2]
    bigM[i] = 2*modDF - n + sumObj$sigma^2 * residDF /sig2hatFull
  }
  best = which.min(bigM)
  if (bigM[best] < smallMallow){
    start = paste(start, ' + ', vecName[vars[best]]) 
    m6 = append(m6, vecName[vars[best]]) 
    vars = vars[-best]
  } else { 
    break
  } 
}

```

**Forward selection (p-val)**
{"hr"   "yr"   "mnth"   "weathersit"   "atemp"   "yr*mnth"   "weekday"   "hum"   "holiday"}

**Forward selection (adjusted R^2) **
{"windspeed"   "holiday"   "hum"   "yr*weekday"   "atemp"   "weathersit"   "yr*mnth"   "yr*hr" }   

**Forward selection (AIC,BIC,Mallows Cp)** 
{"yr*hr"   "yr*mnth"   "weathersit"   "atemp"   "weekday"   "hum"   "holiday"   "windspeed"}


###Selecting Model

Of the three models, we prefer the model that can predict the best. Thus we use cross validation in order to determine which model has the highest probability of predicting future values. Of the three, the forward selection with p-val proved to have the highest probability, even though by a slight value. Thus the model we chose is Model 1. The probabibilities are printed below in order as its listen above.

```{r,  echo = FALSE}
m = c(start2, start3, start4)

permutation <- sample(1:nrow(rntDAT))
folds <- c(rep(1:10, each = nrow(rntDAT)/10))
test_MSE = matrix(0, 10, 3)
#loop over models of each size
for(i in 1:10){
  #identify training and test sets
  idx_train <- permutation[folds != i]
  idx_test <- permutation[folds == i]
  for(j in 1:3){
    
    #extract which variables to use from regfit 
    mod <- lm(m[j], data = rntDAT[idx_train,])
    test_predictions <- predict(mod, rntDAT[idx_test, -11])
    test_MSE[i, j] <- sqrt(mean((rntDAT[idx_test, 11] - test_predictions)^2))
  }
}
apply(test_MSE,2,mean)
```

#Interpretation
Our chosen model is - 

$log(CNT) = A +  b_{hr} hr  +   b_{yr} yr  +  b_{mnth} mnth  +   br_{yxm} yr*mnth  +  br_{wd} weekday  + br_{holiday} holiday  +  b_{ws}  weathersit + b_{atemp} atemp   +   br_{hum} hum$

Thus the total count of rentals procured in an hour is dependent on which hour of the day it is, the month, year, weekday, weather situation outside, how it feels outside, the humidity, whether or not today is a holiday and a interaction between the year and month element. 

There are two types of variables used in our model - continous and categorical variables. The continous variables in the model are ATEMP and HUM. 

* **ATEMP** is the temperature that it feels like outside. This means that if the ATEMP values were to max out at 50, to reach a value of 1, then our model would have a postive increase of 1.433 for the log(CNT). As the temperature increases, the model predicts more rentals.

* **HUM** is the humidity normalized. If the humidity were to max out at 100, to reach a value of 1, our model would decrease by a value 0.26124 for the log(CNT). AS the humidity increase, the model predicts less rentals.

The categorical variables are HR, YR, MNTH, WEEKDAY, HOLIDAY, and WEATHERSIT.

* **HR** is the hour at which the rental was made. The natural value for this variable is set at hour 0 and all our data can be an interpretation relative to midnight of the day. We see an initial decrease in rentals from 12 - 5 AM, but then a sudden increase in rentals from 6 AM to 5 PM. 6 PM and onward the rentals decrease but end off on an overall increase from the day start.

*  **MNTH/YR** is the month and year of whihc the rental was made. The natural setting for this variable is year 0, January. As we move to the year 1, There is an on average increase of 0.78 in log(CNT). As we move from January to the other months, the amount of rentals tend to increase and follow a bimodal trend with a max on May and October. Thus as the years progress and was we travel through the year, the amount of rentals increase. We also included an interaction between MNNTH and YEAR, as we postulated a difference in rentals between the years. Overall, the affect of time on rentals comes out to (0.78 - the coefficient of yr:mnthx + mnthx) for months in year1, an overall increase from the year before. 

* **WEEKDAY** is the weekday from our data set. The natural setting is sunday, and the rest of the data is relative to sunday on the week. We see the most rentals on Friday and Saturday and a mediocre amount of rentals the rest of the week.

* **HOLIDAY** is an indicator variable determining if the day is a holiday or not. Though there were not many holiday variables, the difference between the rentals on non holiday days and holiday days were significant enough to include in our model. The average amount of rentals decrease by 0.1874 when there is a holiday.

* **WEATHERSIT** is an indicator of the weather during the day. The best weather for rentals is waether type 1 -- clear out with few clouds. As the weather progresses through the types, becoming more severe, the amount of bike rentals go down. Something interesting to notice is the lack of a weathersit4 predictor, whihc may be due to the lack of enough samples for this data point.

The last point to interpret is our intercept, which lets us know default values. In the case that we were in year 0, midnight on Sunday in January which is not a holiday but has good weather, with a humidity of 0 and felt like 0 degrees, we predict to have a 2.4279 log(cnt) of sales in that day. 


#Conclusion

Overall I beleive this model makes sense, as the business continues to grow through time the amount of rentals should increase. As well as important factors about the day, whether people tend to work on that day, whether it is a holiday, if the time of the year is good or not, all could be factors for bike rentals for customer. Humidity and the temperature it feels like outside are also important factors for customer's chosing to do a outdoors activity. Thus these variables can appropiately and logically be placed in the model. In further bikeshare use, businesses would benefit from marketing on days that are more favorable to them, as they can predict more bike use on that day.

This model also seems to be the best fit as it has a some of the top R^2, high AIC, BIC, and Mallow's Cp compared to other models and the model we used in the beginning.  Thus through all interpretation, this model seems to be the best fit. 

$log(CNT) = A +  b_{hr} hr  +   b_{yr} yr  +  b_{mnth} mnth  +   br_{yxm} yr*mnth  +  br_{wd} weekday  + br_{holiday} holiday  +  b_{ws}  weathersit + b_{atemp} atemp   +   br_{hum} hum$


\newpage

#APPENDIX

```{}
hist((bike$cnt)^(1/2))
abline(v = mean((bike$cnt)^(1/2)), col = "blue", lwd = 2)
abline(v = median((bike$cnt)^(1/2)), col = "red", lwd = 2)

#determining workday vs weekday vs holiday

mean(rntDAT[which(rntDAT$weekday %in% c(0,6)),]$cnt)
mean(rntDAT[which(rntDAT$weekday %in% c(1,2,3,4,5)),]$cnt)
summary(rntDAT$cnt)
sd(rntDAT$cnt)

#mnth vs season
boxplot(cnt ~  season + mnth , data = bike)
#temp vs atemp
gpairs(bike[,c("temp","atemp", "hum", "windspeed", "weathersit")])
```

```{}
bike <- read.csv("BikeSharingDataset.csv")

#must convert columns to the correct data type in order to use it properly in linear modeling.  
bike[,'dteday']<-as.Date(bike[,'dteday'])
#Factor > Date



bike[,'mnth']<-factor(bike[,'mnth'])
bike[,'hr']<-factor(bike[,'hr'])
bike[,'holiday']<-factor(bike[,'holiday'])
bike[,'weekday']<-factor(bike[,'weekday'])
#bike[,'season']<-factor(bike[,'season'])
bike[,'workingday']<-factor(bike[,'workingday'])
bike[,'weathersit']<-factor(bike[,'weathersit'])
#Ints > Factors

head(bike)

```



```{}
#pairs graph
gpairs(bike[,c("atemp", "hum", "windspeed","temp","weathersit")])

#fully fitted data
full_fit <- lm(cnt ~ . , data = rntDAT)
par(mfrow = c(2, 2))
plot(full_fit)
rntDAT$cnt = log(rntDAT$cnt) #transform
rntDAT = rntDAT[-c(586,9124,8855),] #remove influential points
```



```{}
vars = 1:14 


#basic format for these is to start w a blank model and incrmementally add values to it until the threshold doesnt hold for data
vecName = c(names(rntDAT), "yr*mnth", "yr*hr", "yr*weekday","yr*holiday", "yr*workingday")
m2 = NULL 
start2 = 'cnt ~ 1' 
while(TRUE){
  small = lm(start2, data = rntDAT)
  pvals = array(0, dim = length(vars)) 
  for (i in 1:length(vars)){
    big = lm(paste(start2, ' + ', vecName[vars[i]]), data = rntDAT)
    pvals[i] = anova(small,big)[2,6] 
    if(is.na(pvals)){
      break
    }
  }
  if(is.na(pvals)){
      break
  }
  
  best = which.min(pvals)
  
  if (pvals[best] < 0.05){
    start2 = paste(start2, ' + ', vecName[vars[best]]) 
    m2 = append(m2, vecName[vars[best]]) 
    vars = vars[-best]
  } else { 
    break
  }
}


m3 = NULL 
vars = 1:14
start3 = 'cnt ~ 1' 

while(TRUE){
  smallr2 = summary(lm(start3, data = rntDAT))$adj.r.squared 
  bigr2 = array(0, dim = length(vars))
  for (i in 1:length(vars)){
    bigr2[i] = summary(lm(paste(start3, ' + ', vecName[vars[i]]), data  = rntDAT))$adj.r.squared 
  }
  best = which.max(bigr2)
  if (bigr2[best] > smallr2){
    start3 = paste(start3, ' + ', vecName[vars[best]]) 
    m3 = append(m3, x = vecName[vars[best]]) 
    vars = vars[-best]
  } 
  else { 
    break
  } 
}

m4 = NULL 
vars = 1:14
start4 = 'cnt ~ 1' 
while(TRUE){
  saic = extractAIC(lm(start4, data = rntDAT))[2]
  baic = array(0, dim = length(vars))
  for (i in 1:length(vars)){
    baic[i] = extractAIC(lm(paste(start4, ' + ', vecName[vars[i]]), data = rntDAT))[2] 
  }
  best = which.min(baic)
  if (baic[best] < saic){
    start4= paste(start4, ' + ', vecName[vars[best]]) 
    m4 = append(m4, vecName[vars[best]]) 
    vars = vars[-best]
  } else { 
    break
  } 
}

m5 = NULL 
vars = 1:14
start = 'cnt ~ 1' 
while(TRUE){
  sbic = extractAIC(lm(start, data = rntDAT), k = log(nrow(rntDAT)))[2] 
  bbic = array(0, dim = length(vars))
  for (i in 1:length(vars)){
    bbic[i] = extractAIC(lm(paste(start, ' + ', vecName[vars[i]]), data = rntDAT), k = log(nrow(rntDAT)))[2] 
  }
  best = which.min(bbic)
  if (bbic[best] < sbic){
    start = paste(start, ' + ', vecName[vars[best]]) 
    m5 = append(m5, vecName[vars[best]]) 
    vars = vars[-best]
  } else { 
    break
  } 
}

m6 = NULL 
vars = 1:14
start = 'cnt ~ 1'
sig2hatFull =summary(lm(cnt ~ ., data = rntDAT))$sigma^2
n = nrow(rntDAT)

while(TRUE){
  sumObj =summary(lm(start, data = rntDAT))
  modDF = sumObj$df[1]
  residDF = sumObj$df[2]
  smallMallow = 2*modDF - n + sumObj$sigma^2 * residDF /sig2hatFull 
  bigM = array(0, dim = length(vars))
  for (i in 1:length(vars)){
    sumObj =summary(lm(paste(start, ' + ', vecName[vars[i]]), data = rntDAT))
    modDF = sumObj$df[1]
    residDF = sumObj$df[2]
    bigM[i] = 2*modDF - n + sumObj$sigma^2 * residDF /sig2hatFull
  }
  best = which.min(bigM)
  if (bigM[best] < smallMallow){
    start = paste(start, ' + ', vecName[vars[best]]) 
    m6 = append(m6, vecName[vars[best]]) 
    vars = vars[-best]
  } else { 
    break
  } 
}

```

```{}

#cross validation
m = c(start2, start3, start4)

permutation <- sample(1:nrow(rntDAT))
folds <- c(rep(1:10, each = nrow(rntDAT)/10))
test_MSE = matrix(0, 10, 3)
#loop over models of each size
for(i in 1:10){
  #identify training and test sets
  idx_train <- permutation[folds != i]
  idx_test <- permutation[folds == i]
  for(j in 1:3){
    
    #extract which variables to use from regfit 
    mod <- lm(m[j], data = rntDAT[idx_train,])
    test_predictions <- predict(mod, rntDAT[idx_test, -11])
    test_MSE[i, j] <- sqrt(mean((rntDAT[idx_test, 11] - test_predictions)^2))
  }
}
apply(test_MSE,2,mean)
```
